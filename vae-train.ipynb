{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8585455,"sourceType":"datasetVersion","datasetId":5134949}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nfrom torchvision import transforms\nimport re\nfrom PIL import ImageFile\nfrom tqdm.notebook import tqdm\nHaze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\nHaze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\nRain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\nRain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\nSnow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\nSnow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n\nls_train=[Haze_Train,Rain_Train,Snow_Train]\nindex_train=[1100,3850,5500]\nls_test=[Haze_Test,Rain_Test,Snow_Test]\nindex_test=[160,560,800]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:58:12.845286Z","iopub.execute_input":"2024-06-03T03:58:12.845617Z","iopub.status.idle":"2024-06-03T03:58:18.097728Z","shell.execute_reply.started":"2024-06-03T03:58:12.845591Z","shell.execute_reply":"2024-06-03T03:58:18.096740Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class TrainData(data.Dataset):\n    def __init__(self, crop_size,train_root,train_index,Train=True):\n        super().__init__()\n        self.crop_size=(crop_size,crop_size)\n        self.ind=train_index\n        self.root=train_root\n        \n    def get_images(self, index):\n        \n        if(0<= index<self.ind[0]):\n            in_dir=self.root[0].replace(\"gt\", \"in\")\n            gt_dir=self.root[0]\n            img_id=torch.tensor([0], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1)\n        elif(self.ind[0]<=index<self.ind[1]):\n            in_dir = self.root[1].replace(\"gt\", \"in\")\n            gt_dir = self.root[1]\n            img_id=torch.tensor([1], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[0])\n        elif(self.ind[1]<=index):\n            in_dir = self.root[2].replace(\"gt\", \"in\")\n            gt_dir = self.root[2]\n            img_id=torch.tensor([2], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[1])\n              \n        input_name=tp\n        gt_name=tp\n        try:\n            input_img = Image.open(in_dir + input_name)\n        \n            gt_img = Image.open(gt_dir + gt_name)\n        except:\n            print(index)\n            print(tp)\n        # --- Transform to tensor --- #\n        transform_input =transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        input_im = transform_input(input_img)\n        gt = transform_gt(gt_img)\n        z=gt-input_im\n        return z, input_im, img_id\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return self.ind[-1]\ndef train_load(size=256,batch_size=64,train=True):\n    Haze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\n    Haze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\n    Rain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\n    Rain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\n    Snow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\n    Snow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n    if(train==True):\n        ls_train=[Haze_Train,Rain_Train,Snow_Train]\n        index_train=[1100,3850,5500]\n    else:\n        ls_train=[Haze_Test,Rain_Test,Snow_Test]\n        index_train=[160,560,800]\n    dataset = TrainData(size,ls_train,index_train ,train)\n    Train_load = DataLoader(dataset, batch_size, shuffle=True)\n    return Train_load","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:58:22.549517Z","iopub.execute_input":"2024-06-03T03:58:22.549998Z","iopub.status.idle":"2024-06-03T03:58:22.567659Z","shell.execute_reply.started":"2024-06-03T03:58:22.549969Z","shell.execute_reply":"2024-06-03T03:58:22.566670Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# # # # # # ***Test***","metadata":{}},{"cell_type":"code","source":"Train_load=train_load(224,64)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:21:36.012027Z","iopub.execute_input":"2024-06-02T15:21:36.012431Z","iopub.status.idle":"2024-06-02T15:25:47.294654Z","shell.execute_reply.started":"2024-06-02T15:21:36.012398Z","shell.execute_reply":"2024-06-02T15:25:47.293496Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec64eddce7943e09ea8fc91a2cce8b7"}},"metadata":{}}]},{"cell_type":"code","source":"Train_load=train_load(224,64,train=False)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:28:33.657258Z","iopub.execute_input":"2024-06-02T15:28:33.657669Z","iopub.status.idle":"2024-06-02T15:29:22.756056Z","shell.execute_reply.started":"2024-06-02T15:28:33.657614Z","shell.execute_reply":"2024-06-02T15:29:22.754877Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10fa77321804d658f300879eaca1d1b"}},"metadata":{}}]},{"cell_type":"code","source":"img=TrainData(256,ls_train,index_train)\na,b,c=img.get_images(1110)\n\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:02:07.849382Z","iopub.execute_input":"2024-06-02T14:02:07.849776Z","iopub.status.idle":"2024-06-02T14:02:07.876639Z","shell.execute_reply.started":"2024-06-02T14:02:07.849746Z","shell.execute_reply":"2024-06-02T14:02:07.875711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # # ***Vae Model to Train***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads\n\n        assert (\n                self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n\n    def forward(self, x):\n        values, keys, query=x,x,x\n        N = query.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n        # Split the embedding into self.heads different pieces\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n\n        values = self.values(values)\n        keys = self.keys(keys)\n        queries = self.queries(queries)\n\n        # Scaled dot-product attention\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n            N, query_len, self.heads * self.head_dim\n        )\n        out = self.fc_out(out)\n        return out\n\n\nclass PatchTransformerEncoder(nn.Module):\n    def __init__(self, in_channels, patch_size=10, embedding_dim=128, num_heads=4, use_class_token=False):\n        \"\"\"ViT-like transformer block\n\n        Args:\n            in_channels (int): Input channels\n            patch_size (int, optional): patch size. Defaults to 10.\n            embedding_dim (int, optional): Embedding dimension in transformer model. Defaults to 128.\n            num_heads (int, optional): number of attention heads. Defaults to 4.\n            use_class_token (bool, optional): Whether to use extra token at the start for global accumulation (called as \"class token\"). Defaults to False.\n        \"\"\"\n        super(PatchTransformerEncoder, self).__init__()\n        self.use_class_token = use_class_token\n        encoder_layers = nn.TransformerEncoderLayer(\n            embedding_dim, num_heads, dim_feedforward=1024)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layers, num_layers=4)  # takes shape S,N,E\n        self.patch_size=patch_size\n        self.embedding_convPxP = nn.Conv2d(in_channels, embedding_dim,\n                                           kernel_size=patch_size, stride=patch_size, padding=0)\n\n    def positional_encoding_1d(self, sequence_length, batch_size, embedding_dim, device='cpu'):\n        \"\"\"Generate positional encodings\n\n        Args:\n            sequence_length (int): Sequence length\n            embedding_dim (int): Embedding dimension\n\n        Returns:\n            torch.Tensor SBE: Positional encodings\n        \"\"\"\n        position = torch.arange(\n            0, sequence_length, dtype=torch.float32, device=device).unsqueeze(1)\n        index = torch.arange(\n            0, embedding_dim, 2, dtype=torch.float32, device=device).unsqueeze(0)\n        div_term = torch.exp(index * (-torch.log(torch.tensor(10000.0, device=device)) / embedding_dim))\n        pos_encoding = position * div_term\n        pos_encoding = torch.cat([torch.sin(pos_encoding), torch.cos(pos_encoding)], dim=1)\n        pos_encoding = pos_encoding.unsqueeze(1).repeat(1, batch_size, 1)\n        return pos_encoding\n\n    def forward(self, x):\n        \"\"\"Forward pass\n\n        Args:\n            x (torch.Tensor - NCHW): Input feature tensor\n\n        Returns:\n            torch.Tensor - SNE: Transformer output embeddings. S - sequence length (=HW/patch_size^2), N - batch size, E - embedding dim\n        \"\"\"\n        B,C,H,W=x.shape\n        embeddings = self.embedding_convPxP(x).flatten(\n            2)  # .shape = n,c,s = n, embedding_dim, s\n        if self.use_class_token:\n            # extra special token at start ?\n            embeddings = nn.functional.pad(embeddings, (1, 0))\n\n        # change to S,N,E format required by transformer\n        embeddings = embeddings.permute(2, 0, 1)\n        S, N, E = embeddings.shape\n        embeddings = embeddings + self.positional_encoding_1d(S, N, E, device=embeddings.device)\n        x = self.transformer_encoder(embeddings)  # .shape = S, N, E\n        x=x.permute(1,2,0).reshape(B,-1,self.patch_size,self.patch_size,H//self.patch_size,W//self.patch_size)\n        x=x.permute(0,1,2,4,3,5).reshape(B,-1,H,W)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n        self.conv1_resn=nn.Conv2d(3, 64, kernel_size=1, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2_resn = nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0)\n\n        self.patch_size=8\n        self.attn = PatchTransformerEncoder(64, self.patch_size, 1024)\n        self.proj_attn=nn.Conv2d(64,16,kernel_size=3,stride=1,padding=1)\n\n        self.conv4 = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)\n        self.conv4_resn=nn.Conv2d(16, 3, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x)+self.conv1_resn(x))\n        B, C, H, W = x.shape\n        x = self.attn(x) + self.proj_attn(x)\n        x = F.relu(self.conv2(x)+self.conv2_resn(x))\n\n        x = F.relu(self.conv4(x)+self.conv4_resn(x))\n        return x\nclass OverlapPatchEmbed(nn.Module):\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = (img_size,img_size)\n        patch_size = (patch_size,patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # pdb.set_trace()\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n\nclass Classify(nn.Module):\n    def __init__(self,img_size=64,in_chans=3,embed_dims=[32,32,8,8],heads=[8,4,4,4]):\n        super(Classify, self).__init__()\n        self.embed=embed_dims\n        self.image_size=img_size\n\n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=3, stride=2, in_chans=in_chans,\n                                              embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n\n        self.mini_patch_embed1 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[0],\n                                                   embed_dim=embed_dims[1])\n        self.mini_patch_embed2 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[1],\n                                                   embed_dim=embed_dims[2])\n\n        self.self_atten1=SelfAttention(embed_dims[0],heads[0])\n        self.self_atten2=SelfAttention(embed_dims[1],heads[1])\n        self.self_atten3 = SelfAttention(embed_dims[2], heads[2])\n        self.linear_1=nn.Linear(512,100)\n        self.linear_2=nn.Linear(100,3)\n    def forward(self,x):\n        B = x.shape[0]\n        embed_dims = self.embed\n        x1, H1, W1 = self.patch_embed1(x)\n        x2, H2, W2 = self.mini_patch_embed1(x1.permute(0, 2, 1).reshape(B, embed_dims[0], H1, W1))\n\n        x1=self.self_atten1(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten2(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed2(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[1], H1, W1) + x2\n        x2, H2, W2 = self.mini_patch_embed2(x1)\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1=self.self_atten2(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten3(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed3(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[2], H1, W1) + x2\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1 = self.self_atten3(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1=x1.flatten(1)\n        tp=x1\n        x1=self.linear_1(x1)\n        x1=self.linear_2(x1)\n        return x1,tp\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.conv1=nn.Conv2d(3,32,kernel_size=3,stride=1,padding=1)\n        self.deconv1 = nn.ConvTranspose2d(32,16, kernel_size=4, stride=2, padding=1)\n\n        self.attn = PatchTransformerEncoder(32,4,512,4)\n\n        self.deconv4 = nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1)\n\n    def forward(self, x):\n\n        x=self.conv1(x)\n\n        x = self.attn(x) + x\n\n        x = F.relu(self.deconv1(x))\n\n        x = torch.sigmoid(self.deconv4(x))\n        return x\n\n\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.classify=Classify()\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_recon = self.decoder(z)\n        x_classify,embed=self.classify(z)\n        return x_recon,x_classify,embed\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:58:27.701832Z","iopub.execute_input":"2024-06-03T03:58:27.702227Z","iopub.status.idle":"2024-06-03T03:58:27.753966Z","shell.execute_reply.started":"2024-06-03T03:58:27.702200Z","shell.execute_reply":"2024-06-03T03:58:27.753111Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # ***test_model***","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndevice=\"cuda\"\nvae = VAE().to(device)\ninput_tensor = torch.randn(64, 3, 256, 256).to(device)\noutput_tensor,class_tp,embed_tp = vae(input_tensor)\nprint(output_tensor.shape)\nprint(class_tp.shape)\nprint(embed_tp.shape)\nprint(f\"显存使用 (分配): {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f} MB\")\nprint(f\"显存使用 (保留): {torch.cuda.memory_reserved(device) / 1024 ** 2:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:51:47.511089Z","iopub.execute_input":"2024-06-03T03:51:47.511442Z","iopub.status.idle":"2024-06-03T03:51:48.290105Z","shell.execute_reply.started":"2024-06-03T03:51:47.511407Z","shell.execute_reply":"2024-06-03T03:51:48.288892Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 256, 256])\ntorch.Size([64, 3])\ntorch.Size([64, 512])\n显存使用 (分配): 7790.49 MB\n显存使用 (保留): 12574.00 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"del(vae)\ndel(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:52:23.790596Z","iopub.execute_input":"2024-06-03T03:52:23.791400Z","iopub.status.idle":"2024-06-03T03:52:23.795657Z","shell.execute_reply.started":"2024-06-03T03:52:23.791368Z","shell.execute_reply":"2024-06-03T03:52:23.794676Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Train Officially***","metadata":{}},{"cell_type":"code","source":"epoch=0\nmax_epoch=100\ndataset=train_load(batch_size=32)\nnet=VAE()\ndevice=\"cuda\"\noptimizer=optim.Adam(net.parameters(),lr=0.01)\nMse_loss=nn.MSELoss()\nGp_loss=nn.CrossEntropyLoss()\nalpha=0.01\nnet.to(device)\nnet.train()\nwhile(epoch<max_epoch):\n    phar=tqdm(total=len(dataset))\n\n    for step,(resn,gt,label) in enumerate(dataset):\n        x=resn\n        x=x.to(device)\n        label=label.view(-1).to(device)\n        optimizer.zero_grad()\n\n        x_recon,pred_label,embed=net(x)\n        loss1=alpha*Mse_loss(x,x_recon)\n        loss2=Gp_loss(pred_label,label)\n        loss=loss2+loss1\n        loss.backward()\n        optimizer.step()\n        phar.set_description(\n            \"epoch:{} loss:{}  \".format(epoch, loss ))\n        phar.update()\n    phar.close()\n    epoch+=1","metadata":{"execution":{"iopub.status.busy":"2024-06-03T04:01:41.939983Z","iopub.execute_input":"2024-06-03T04:01:41.940338Z","iopub.status.idle":"2024-06-03T04:02:56.185021Z","shell.execute_reply.started":"2024-06-03T04:01:41.940313Z","shell.execute_reply":"2024-06-03T04:02:56.183701Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461d6d917a3043eb9032a05c94118114"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m loss2\u001b[38;5;241m=\u001b[39mGp_loss(pred_label,label)\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss2\u001b[38;5;241m+\u001b[39mloss1\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m phar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss ))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for step,(resn,gt,label) in enumerate(dataset):\n    print(label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}