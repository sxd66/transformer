{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8585455,"sourceType":"datasetVersion","datasetId":5134949}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nfrom torchvision import transforms\nimport re\nfrom PIL import ImageFile\nfrom tqdm.notebook import tqdm\nHaze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\nHaze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\nRain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\nRain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\nSnow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\nSnow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n\nls_train=[Haze_Train,Rain_Train,Snow_Train]\nindex_train=[1100,3850,5500]\nls_test=[Haze_Test,Rain_Test,Snow_Test]\nindex_test=[160,560,800]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:11:15.405559Z","iopub.execute_input":"2024-06-03T13:11:15.405939Z","iopub.status.idle":"2024-06-03T13:11:20.763597Z","shell.execute_reply.started":"2024-06-03T13:11:15.405908Z","shell.execute_reply":"2024-06-03T13:11:20.762594Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class TrainData(data.Dataset):\n    def __init__(self, crop_size,train_root,train_index,Train=True):\n        super().__init__()\n        self.crop_size=(crop_size,crop_size)\n        self.ind=train_index\n        self.root=train_root\n        \n    def get_images(self, index):\n        \n        if(0<= index<self.ind[0]):\n            in_dir=self.root[0].replace(\"gt\", \"in\")\n            gt_dir=self.root[0]\n            img_id=torch.tensor([0], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1)\n        elif(self.ind[0]<=index<self.ind[1]):\n            in_dir = self.root[1].replace(\"gt\", \"in\")\n            gt_dir = self.root[1]\n            img_id=torch.tensor([1], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[0])\n        elif(self.ind[1]<=index):\n            in_dir = self.root[2].replace(\"gt\", \"in\")\n            gt_dir = self.root[2]\n            img_id=torch.tensor([2], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[1])\n              \n        input_name=tp\n        gt_name=tp\n        try:\n            input_img = Image.open(in_dir + input_name)\n        \n            gt_img = Image.open(gt_dir + gt_name)\n        except:\n            print(index)\n            print(tp)\n        # --- Transform to tensor --- #\n        transform_input =transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        input_im = transform_input(input_img)\n        gt = transform_gt(gt_img)\n        z=gt-input_im\n        return z, input_im, img_id\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return self.ind[-1]\ndef train_load(size=256,batch_size=64,train=True):\n    Haze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\n    Haze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\n    Rain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\n    Rain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\n    Snow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\n    Snow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n    if(train==True):\n        ls_train=[Haze_Train,Rain_Train,Snow_Train]\n        index_train=[1100,3850,5500]\n    else:\n        ls_train=[Haze_Test,Rain_Test,Snow_Test]\n        index_train=[160,560,800]\n    dataset = TrainData(size,ls_train,index_train ,train)\n    Train_load = DataLoader(dataset, batch_size, shuffle=True)\n    return Train_load","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:11:43.741929Z","iopub.execute_input":"2024-06-03T13:11:43.742257Z","iopub.status.idle":"2024-06-03T13:11:43.758712Z","shell.execute_reply.started":"2024-06-03T13:11:43.742232Z","shell.execute_reply":"2024-06-03T13:11:43.757725Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# # # # # # ***Test***","metadata":{}},{"cell_type":"code","source":"Train_load=train_load(224,64)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:21:36.012027Z","iopub.execute_input":"2024-06-02T15:21:36.012431Z","iopub.status.idle":"2024-06-02T15:25:47.294654Z","shell.execute_reply.started":"2024-06-02T15:21:36.012398Z","shell.execute_reply":"2024-06-02T15:25:47.293496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_load=train_load(224,64,train=False)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:28:33.657258Z","iopub.execute_input":"2024-06-02T15:28:33.657669Z","iopub.status.idle":"2024-06-02T15:29:22.756056Z","shell.execute_reply.started":"2024-06-02T15:28:33.657614Z","shell.execute_reply":"2024-06-02T15:29:22.754877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=TrainData(256,ls_train,index_train)\na,b,c=img.get_images(1110)\n\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:02:07.849382Z","iopub.execute_input":"2024-06-02T14:02:07.849776Z","iopub.status.idle":"2024-06-02T14:02:07.876639Z","shell.execute_reply.started":"2024-06-02T14:02:07.849746Z","shell.execute_reply":"2024-06-02T14:02:07.875711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # # ***Vae Model to Train***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads\n\n        assert (\n                self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n\n    def forward(self, x):\n        values, keys, query=x,x,x\n        N = query.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n        # Split the embedding into self.heads different pieces\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n\n        values = self.values(values)\n        keys = self.keys(keys)\n        queries = self.queries(queries)\n\n        # Scaled dot-product attention\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n            N, query_len, self.heads * self.head_dim\n        )\n        out = self.fc_out(out)\n        return out\n\n\nclass PatchTransformerEncoder(nn.Module):\n    def __init__(self, in_channels, patch_size=10, embedding_dim=128, num_heads=4, use_class_token=False):\n        \"\"\"ViT-like transformer block\n\n        Args:\n            in_channels (int): Input channels\n            patch_size (int, optional): patch size. Defaults to 10.\n            embedding_dim (int, optional): Embedding dimension in transformer model. Defaults to 128.\n            num_heads (int, optional): number of attention heads. Defaults to 4.\n            use_class_token (bool, optional): Whether to use extra token at the start for global accumulation (called as \"class token\"). Defaults to False.\n        \"\"\"\n        super(PatchTransformerEncoder, self).__init__()\n        self.use_class_token = use_class_token\n        encoder_layers = nn.TransformerEncoderLayer(\n            embedding_dim, num_heads, dim_feedforward=1024)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layers, num_layers=4)  # takes shape S,N,E\n        self.patch_size=patch_size\n        self.embedding_convPxP = nn.Conv2d(in_channels, embedding_dim,\n                                           kernel_size=patch_size, stride=patch_size, padding=0)\n\n    def positional_encoding_1d(self, sequence_length, batch_size, embedding_dim, device='cpu'):\n        \"\"\"Generate positional encodings\n\n        Args:\n            sequence_length (int): Sequence length\n            embedding_dim (int): Embedding dimension\n\n        Returns:\n            torch.Tensor SBE: Positional encodings\n        \"\"\"\n        position = torch.arange(\n            0, sequence_length, dtype=torch.float32, device=device).unsqueeze(1)\n        index = torch.arange(\n            0, embedding_dim, 2, dtype=torch.float32, device=device).unsqueeze(0)\n        div_term = torch.exp(index * (-torch.log(torch.tensor(10000.0, device=device)) / embedding_dim))\n        pos_encoding = position * div_term\n        pos_encoding = torch.cat([torch.sin(pos_encoding), torch.cos(pos_encoding)], dim=1)\n        pos_encoding = pos_encoding.unsqueeze(1).repeat(1, batch_size, 1)\n        return pos_encoding\n\n    def forward(self, x):\n        \"\"\"Forward pass\n\n        Args:\n            x (torch.Tensor - NCHW): Input feature tensor\n\n        Returns:\n            torch.Tensor - SNE: Transformer output embeddings. S - sequence length (=HW/patch_size^2), N - batch size, E - embedding dim\n        \"\"\"\n        B,C,H,W=x.shape\n        embeddings = self.embedding_convPxP(x).flatten(\n            2)  # .shape = n,c,s = n, embedding_dim, s\n        if self.use_class_token:\n            # extra special token at start ?\n            embeddings = nn.functional.pad(embeddings, (1, 0))\n\n        # change to S,N,E format required by transformer\n        embeddings = embeddings.permute(2, 0, 1)\n        S, N, E = embeddings.shape\n        embeddings = embeddings + self.positional_encoding_1d(S, N, E, device=embeddings.device)\n        x = self.transformer_encoder(embeddings)  # .shape = S, N, E\n        x=x.permute(1,2,0).reshape(B,-1,self.patch_size,self.patch_size,H//self.patch_size,W//self.patch_size)\n        x=x.permute(0,1,2,4,3,5).reshape(B,-1,H,W)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n        self.conv1_resn=nn.Conv2d(3, 64, kernel_size=1, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2_resn = nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0)\n\n        self.patch_size=8\n        self.attn = PatchTransformerEncoder(64, self.patch_size, 1024)\n        self.proj_attn=nn.Conv2d(64,16,kernel_size=3,stride=1,padding=1)\n\n        self.conv4 = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)\n        self.conv4_resn=nn.Conv2d(16, 3, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x)+self.conv1_resn(x))\n        B, C, H, W = x.shape\n        x = self.attn(x) + self.proj_attn(x)\n        x = F.relu(self.conv2(x)+self.conv2_resn(x))\n\n        x = F.relu(self.conv4(x)+self.conv4_resn(x))\n        return x\nclass OverlapPatchEmbed(nn.Module):\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = (img_size,img_size)\n        patch_size = (patch_size,patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # pdb.set_trace()\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n\nclass Classify(nn.Module):\n    def __init__(self,img_size=64,in_chans=3,embed_dims=[32,32,8,8],heads=[8,4,4,4]):\n        super(Classify, self).__init__()\n        self.embed=embed_dims\n        self.image_size=img_size\n\n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=3, stride=2, in_chans=in_chans,\n                                              embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n\n        self.mini_patch_embed1 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[0],\n                                                   embed_dim=embed_dims[1])\n        self.mini_patch_embed2 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[1],\n                                                   embed_dim=embed_dims[2])\n\n        self.self_atten1=SelfAttention(embed_dims[0],heads[0])\n        self.self_atten2=SelfAttention(embed_dims[1],heads[1])\n        self.self_atten3 = SelfAttention(embed_dims[2], heads[2])\n        self.linear_1=nn.Linear(512,100)\n        self.linear_2=nn.Linear(100,3)\n    def forward(self,x):\n        B = x.shape[0]\n        embed_dims = self.embed\n        x1, H1, W1 = self.patch_embed1(x)\n        x2, H2, W2 = self.mini_patch_embed1(x1.permute(0, 2, 1).reshape(B, embed_dims[0], H1, W1))\n\n        x1=self.self_atten1(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten2(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed2(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[1], H1, W1) + x2\n        x2, H2, W2 = self.mini_patch_embed2(x1)\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1=self.self_atten2(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten3(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed3(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[2], H1, W1) + x2\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1 = self.self_atten3(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1=x1.flatten(1)\n        tp=x1\n        x1=self.linear_1(x1)\n        x1=self.linear_2(x1)\n        return x1,tp\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.conv1=nn.Conv2d(3,32,kernel_size=3,stride=1,padding=1)\n        self.deconv1 = nn.ConvTranspose2d(32,16, kernel_size=4, stride=2, padding=1)\n\n        self.attn = PatchTransformerEncoder(32,4,512,4)\n\n        self.deconv4 = nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1)\n\n    def forward(self, x):\n\n        x=self.conv1(x)\n\n        x = self.attn(x) + x\n\n        x = F.relu(self.deconv1(x))\n\n        x = torch.sigmoid(self.deconv4(x))\n        return x\n\n\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.classify=Classify()\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_recon = self.decoder(z)\n        x_classify,embed=self.classify(z)\n        return x_recon,x_classify,embed\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:11:50.879181Z","iopub.execute_input":"2024-06-03T13:11:50.879521Z","iopub.status.idle":"2024-06-03T13:11:50.947852Z","shell.execute_reply.started":"2024-06-03T13:11:50.879495Z","shell.execute_reply":"2024-06-03T13:11:50.946914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Load Data***","metadata":{}},{"cell_type":"code","source":"path=\"/kaggle/input/train-data/latest (1)\"\nmodel=VAE()\nwith open(path, \"rb\") as f:\n    state=torch.load(f, map_location=\"cpu\")\nmodel.load_state_dict(state[\"model\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:40:44.488474Z","iopub.execute_input":"2024-06-03T12:40:44.488861Z","iopub.status.idle":"2024-06-03T12:40:46.918041Z","shell.execute_reply.started":"2024-06-03T12:40:44.488831Z","shell.execute_reply":"2024-06-03T12:40:46.916800Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"y,a,b=model(torch.randn(1,3,256,256))\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:41:42.992060Z","iopub.execute_input":"2024-06-03T12:41:42.992818Z","iopub.status.idle":"2024-06-03T12:41:43.449315Z","shell.execute_reply.started":"2024-06-03T12:41:42.992783Z","shell.execute_reply":"2024-06-03T12:41:43.448234Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 256, 256])"},"metadata":{}}]},{"cell_type":"code","source":"print(state[\"loss1\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:00:15.140438Z","iopub.execute_input":"2024-06-03T13:00:15.140864Z","iopub.status.idle":"2024-06-03T13:00:15.159776Z","shell.execute_reply.started":"2024-06-03T13:00:15.140831Z","shell.execute_reply":"2024-06-03T13:00:15.158287Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor(0.3757, requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# # # ***test_model***","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndevice=\"cuda\"\nvae = VAE().to(device)\ninput_tensor = torch.randn(64, 3, 256, 256).to(device)\noutput_tensor,class_tp,embed_tp = vae(input_tensor)\nprint(output_tensor.shape)\nprint(class_tp.shape)\nprint(embed_tp.shape)\nprint(f\"显存使用 (分配): {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f} MB\")\nprint(f\"显存使用 (保留): {torch.cuda.memory_reserved(device) / 1024 ** 2:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:51:47.511089Z","iopub.execute_input":"2024-06-03T03:51:47.511442Z","iopub.status.idle":"2024-06-03T03:51:48.290105Z","shell.execute_reply.started":"2024-06-03T03:51:47.511407Z","shell.execute_reply":"2024-06-03T03:51:48.288892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(vae)\ndel(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:52:23.790596Z","iopub.execute_input":"2024-06-03T03:52:23.791400Z","iopub.status.idle":"2024-06-03T03:52:23.795657Z","shell.execute_reply.started":"2024-06-03T03:52:23.791368Z","shell.execute_reply":"2024-06-03T03:52:23.794676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Train Officially***","metadata":{}},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:12:45.567491Z","iopub.execute_input":"2024-06-03T13:12:45.568221Z","iopub.status.idle":"2024-06-03T13:12:45.572949Z","shell.execute_reply.started":"2024-06-03T13:12:45.568187Z","shell.execute_reply":"2024-06-03T13:12:45.571868Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"***lr hanshu***","metadata":{}},{"cell_type":"code","source":"epoch=0\nmax_epoch=100\ndataset=train_load(batch_size=32)\nval_data=train_load(batch_size=32,train=False)\nnet=VAE()\ndevice=\"cuda\"\noptimizer=optim.Adam(net.parameters(),lr=0.001)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n##load_checkpoint\npath=\"/kaggle/input/train-data/latest (1)\"\nwith open(path, \"rb\") as f:\n    state=torch.load(f, map_location=\"cpu\")\nnet.load_state_dict(state[\"model\"])\n\nMse_loss=nn.MSELoss()\nGp_loss=nn.CrossEntropyLoss()\nlambda_recon = 0.8\nlambda_cls = 0.2\nexp_name=\"vae\"\nos.makedirs(\"./{}\".format(exp_name))\n\nnet.to(device)\nnet.train()\nwhile(epoch<max_epoch):\n    phar=tqdm(total=len(dataset))\n    loss_avg=0\n    z = get_lr(optimizer)\n    for step,(resn,input_img,label) in enumerate(dataset):\n        x=resn\n        x=x.to(device)\n        label=label.view(-1).to(device)\n        optimizer.zero_grad()\n        \n        x_recon,pred_label,embed=net(x)\n        loss1=Mse_loss(x,x_recon)\n        loss2=Gp_loss(pred_label,label)\n        \n        loss=loss1*lambda_recon+loss2*lambda_cls\n        loss_avg=(loss_avg*step+loss)/(step+1)\n        loss.backward()\n        optimizer.step()\n        phar.set_description(\n            \"epoch:{} loss:{:.4f} mse_loss:{:.4f} class_loss:{:.4f} lr:{}\".format(epoch, loss_avg,loss1,loss2,z ))\n        phar.update()\n        \"\"\"\n        try:\n            if(step%51==0):\n                print(pred_label[:10])\n                print(label[:10])\n        except:\n            pass\n        \"\"\"\n    phar.close()\n    \n    loss_avg=0\n    loss_cls=0\n    loss_recon=0\n    phar=tqdm(total=len(val_data))\n    net.eval()\n    with torch.no_grad():\n        for step,(resn,input_img,label) in enumerate(val_data):\n            x=resn\n            x=x.to(device)\n            label=label.view(-1).to(device)\n\n            x_recon,pred_label,embed=net(x)\n            loss1=Mse_loss(x,x_recon)\n            loss2=Gp_loss(pred_label,label)\n            loss_cls=(loss_cls*step+loss2)/(step+1)\n            loss_recon=(loss_recon*step+loss2)/(step+1)\n            loss=0.8*loss1+0.2*loss2\n            loss_avg=(loss_avg*step+loss)/(step+1)\n            phar.set_description(\n            \"Val: loss:{:.4f} mse_loss:{:.4f} class_loss:{:.4f} \".format( loss_avg,loss_recon,loss_cls ))\n            phar.update()\n    phar.close()\n    epoch+=1\n    state = {\n        \"epoch\": epoch,\n        \"model\": net.state_dict(),\n        \"loss1\": loss1,\n        \"loss2\":loss_avg,\n    }\n    scheduler.step()\n        \n    with open('./{}/latest'.format(exp_name), \"wb\") as f:\n        torch.save(state, f)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T13:14:12.498471Z","iopub.execute_input":"2024-06-03T13:14:12.499069Z","iopub.status.idle":"2024-06-03T13:37:46.931344Z","shell.execute_reply.started":"2024-06-03T13:14:12.499040Z","shell.execute_reply":"2024-06-03T13:37:46.930085Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d390d2b73244b3c95bf219283846cde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c125f63988004affa4ceac104785a9f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7079a8c6b9914e62ab76d405e71d8255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f515397b6d84e739c171785d77475a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6db9ab0981464eaac07e2780b41002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c9fd4bd4d043eda46ba2574c28a6a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98575ad85c514912be49d25a4faea6a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5083147ccc402c8cc0029746a3000b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3da8b0322804bbf97c926632e62e6d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ec9d4a181946889ee06874ec2ece61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd2f888e20a4ea0af5aa5f7e3b69399"}},"metadata":{}},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for step,(resn,gt,label) in enumerate(dataset):\n    print(label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Produse path***","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"./temp\")\nstate=torch.Tensor([1])\nwith open('./{}/latest'.format(\"temp\"), \"wb\") as f:\n        torch.save(state, f)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T07:40:17.026875Z","iopub.execute_input":"2024-06-03T07:40:17.027787Z","iopub.status.idle":"2024-06-03T07:40:17.033202Z","shell.execute_reply.started":"2024-06-03T07:40:17.027753Z","shell.execute_reply":"2024-06-03T07:40:17.032281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.rmdir(\"/kaggle/working/vae\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:16:00.560848Z","iopub.execute_input":"2024-06-03T09:16:00.561228Z","iopub.status.idle":"2024-06-03T09:16:00.568539Z","shell.execute_reply.started":"2024-06-03T09:16:00.561198Z","shell.execute_reply":"2024-06-03T09:16:00.567621Z"},"trusted":true},"execution_count":null,"outputs":[]}]}