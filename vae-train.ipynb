{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8585455,"sourceType":"datasetVersion","datasetId":5134949}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nfrom torchvision import transforms\nimport re\nfrom PIL import ImageFile\nfrom tqdm.notebook import tqdm\nHaze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\nHaze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\nRain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\nRain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\nSnow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\nSnow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n\nls_train=[Haze_Train,Rain_Train,Snow_Train]\nindex_train=[1100,3850,5500]\nls_test=[Haze_Test,Rain_Test,Snow_Test]\nindex_test=[160,560,800]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:13:02.443109Z","iopub.execute_input":"2024-06-03T09:13:02.443477Z","iopub.status.idle":"2024-06-03T09:13:07.846323Z","shell.execute_reply.started":"2024-06-03T09:13:02.443446Z","shell.execute_reply":"2024-06-03T09:13:07.845426Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class TrainData(data.Dataset):\n    def __init__(self, crop_size,train_root,train_index,Train=True):\n        super().__init__()\n        self.crop_size=(crop_size,crop_size)\n        self.ind=train_index\n        self.root=train_root\n        \n    def get_images(self, index):\n        \n        if(0<= index<self.ind[0]):\n            in_dir=self.root[0].replace(\"gt\", \"in\")\n            gt_dir=self.root[0]\n            img_id=torch.tensor([0], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1)\n        elif(self.ind[0]<=index<self.ind[1]):\n            in_dir = self.root[1].replace(\"gt\", \"in\")\n            gt_dir = self.root[1]\n            img_id=torch.tensor([1], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[0])\n        elif(self.ind[1]<=index):\n            in_dir = self.root[2].replace(\"gt\", \"in\")\n            gt_dir = self.root[2]\n            img_id=torch.tensor([2], dtype=torch.int64)\n            tp=\"//{}.png\".format(index+1-self.ind[1])\n              \n        input_name=tp\n        gt_name=tp\n        try:\n            input_img = Image.open(in_dir + input_name)\n        \n            gt_img = Image.open(gt_dir + gt_name)\n        except:\n            print(index)\n            print(tp)\n        # --- Transform to tensor --- #\n        transform_input =transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = transforms.Compose([transforms.Resize(self.crop_size),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        input_im = transform_input(input_img)\n        gt = transform_gt(gt_img)\n        z=gt-input_im\n        return z, input_im, img_id\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return self.ind[-1]\ndef train_load(size=256,batch_size=64,train=True):\n    Haze_Train=\"/kaggle/input/adverse-weather/haze/haze/train/gt\"\n    Haze_Test=\"/kaggle/input/adverse-weather/haze/haze/test/gt\"\n    Rain_Train=\"/kaggle/input/adverse-weather/rain/rain/train/gt\"\n    Rain_Test=\"/kaggle/input/adverse-weather/rain/rain/test/gt\"\n    Snow_Train=\"/kaggle/input/adverse-weather/rain/snow/train/gt\"\n    Snow_Test=\"/kaggle/input/adverse-weather/rain/snow/test/gt\"\n    if(train==True):\n        ls_train=[Haze_Train,Rain_Train,Snow_Train]\n        index_train=[1100,3850,5500]\n    else:\n        ls_train=[Haze_Test,Rain_Test,Snow_Test]\n        index_train=[160,560,800]\n    dataset = TrainData(size,ls_train,index_train ,train)\n    Train_load = DataLoader(dataset, batch_size, shuffle=True)\n    return Train_load","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:14:49.991393Z","iopub.execute_input":"2024-06-03T09:14:49.991949Z","iopub.status.idle":"2024-06-03T09:14:50.010453Z","shell.execute_reply.started":"2024-06-03T09:14:49.991917Z","shell.execute_reply":"2024-06-03T09:14:50.009317Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# # # # # # ***Test***","metadata":{}},{"cell_type":"code","source":"Train_load=train_load(224,64)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:21:36.012027Z","iopub.execute_input":"2024-06-02T15:21:36.012431Z","iopub.status.idle":"2024-06-02T15:25:47.294654Z","shell.execute_reply.started":"2024-06-02T15:21:36.012398Z","shell.execute_reply":"2024-06-02T15:25:47.293496Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec64eddce7943e09ea8fc91a2cce8b7"}},"metadata":{}}]},{"cell_type":"code","source":"Train_load=train_load(224,64,train=False)\nphar=tqdm(total=len(Train_load))\nfor i,data_tp in enumerate(Train_load):\n    img,img_gt,number=data_tp\n    phar.update(1)\nphar.close()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:28:33.657258Z","iopub.execute_input":"2024-06-02T15:28:33.657669Z","iopub.status.idle":"2024-06-02T15:29:22.756056Z","shell.execute_reply.started":"2024-06-02T15:28:33.657614Z","shell.execute_reply":"2024-06-02T15:29:22.754877Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10fa77321804d658f300879eaca1d1b"}},"metadata":{}}]},{"cell_type":"code","source":"img=TrainData(256,ls_train,index_train)\na,b,c=img.get_images(1110)\n\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:02:07.849382Z","iopub.execute_input":"2024-06-02T14:02:07.849776Z","iopub.status.idle":"2024-06-02T14:02:07.876639Z","shell.execute_reply.started":"2024-06-02T14:02:07.849746Z","shell.execute_reply":"2024-06-02T14:02:07.875711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # # ***Vae Model to Train***","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, embed_size, heads):\n        super(SelfAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads\n\n        assert (\n                self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"\n\n        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n\n    def forward(self, x):\n        values, keys, query=x,x,x\n        N = query.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n        # Split the embedding into self.heads different pieces\n        values = values.reshape(N, value_len, self.heads, self.head_dim)\n        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n\n        values = self.values(values)\n        keys = self.keys(keys)\n        queries = self.queries(queries)\n\n        # Scaled dot-product attention\n        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n\n        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n            N, query_len, self.heads * self.head_dim\n        )\n        out = self.fc_out(out)\n        return out\n\n\nclass PatchTransformerEncoder(nn.Module):\n    def __init__(self, in_channels, patch_size=10, embedding_dim=128, num_heads=4, use_class_token=False):\n        \"\"\"ViT-like transformer block\n\n        Args:\n            in_channels (int): Input channels\n            patch_size (int, optional): patch size. Defaults to 10.\n            embedding_dim (int, optional): Embedding dimension in transformer model. Defaults to 128.\n            num_heads (int, optional): number of attention heads. Defaults to 4.\n            use_class_token (bool, optional): Whether to use extra token at the start for global accumulation (called as \"class token\"). Defaults to False.\n        \"\"\"\n        super(PatchTransformerEncoder, self).__init__()\n        self.use_class_token = use_class_token\n        encoder_layers = nn.TransformerEncoderLayer(\n            embedding_dim, num_heads, dim_feedforward=1024)\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layers, num_layers=4)  # takes shape S,N,E\n        self.patch_size=patch_size\n        self.embedding_convPxP = nn.Conv2d(in_channels, embedding_dim,\n                                           kernel_size=patch_size, stride=patch_size, padding=0)\n\n    def positional_encoding_1d(self, sequence_length, batch_size, embedding_dim, device='cpu'):\n        \"\"\"Generate positional encodings\n\n        Args:\n            sequence_length (int): Sequence length\n            embedding_dim (int): Embedding dimension\n\n        Returns:\n            torch.Tensor SBE: Positional encodings\n        \"\"\"\n        position = torch.arange(\n            0, sequence_length, dtype=torch.float32, device=device).unsqueeze(1)\n        index = torch.arange(\n            0, embedding_dim, 2, dtype=torch.float32, device=device).unsqueeze(0)\n        div_term = torch.exp(index * (-torch.log(torch.tensor(10000.0, device=device)) / embedding_dim))\n        pos_encoding = position * div_term\n        pos_encoding = torch.cat([torch.sin(pos_encoding), torch.cos(pos_encoding)], dim=1)\n        pos_encoding = pos_encoding.unsqueeze(1).repeat(1, batch_size, 1)\n        return pos_encoding\n\n    def forward(self, x):\n        \"\"\"Forward pass\n\n        Args:\n            x (torch.Tensor - NCHW): Input feature tensor\n\n        Returns:\n            torch.Tensor - SNE: Transformer output embeddings. S - sequence length (=HW/patch_size^2), N - batch size, E - embedding dim\n        \"\"\"\n        B,C,H,W=x.shape\n        embeddings = self.embedding_convPxP(x).flatten(\n            2)  # .shape = n,c,s = n, embedding_dim, s\n        if self.use_class_token:\n            # extra special token at start ?\n            embeddings = nn.functional.pad(embeddings, (1, 0))\n\n        # change to S,N,E format required by transformer\n        embeddings = embeddings.permute(2, 0, 1)\n        S, N, E = embeddings.shape\n        embeddings = embeddings + self.positional_encoding_1d(S, N, E, device=embeddings.device)\n        x = self.transformer_encoder(embeddings)  # .shape = S, N, E\n        x=x.permute(1,2,0).reshape(B,-1,self.patch_size,self.patch_size,H//self.patch_size,W//self.patch_size)\n        x=x.permute(0,1,2,4,3,5).reshape(B,-1,H,W)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n        self.conv1_resn=nn.Conv2d(3, 64, kernel_size=1, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2_resn = nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=0)\n\n        self.patch_size=8\n        self.attn = PatchTransformerEncoder(64, self.patch_size, 1024)\n        self.proj_attn=nn.Conv2d(64,16,kernel_size=3,stride=1,padding=1)\n\n        self.conv4 = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)\n        self.conv4_resn=nn.Conv2d(16, 3, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x)+self.conv1_resn(x))\n        B, C, H, W = x.shape\n        x = self.attn(x) + self.proj_attn(x)\n        x = F.relu(self.conv2(x)+self.conv2_resn(x))\n\n        x = F.relu(self.conv4(x)+self.conv4_resn(x))\n        return x\nclass OverlapPatchEmbed(nn.Module):\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = (img_size,img_size)\n        patch_size = (patch_size,patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        # pdb.set_trace()\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n        return x, H, W\n\nclass Classify(nn.Module):\n    def __init__(self,img_size=64,in_chans=3,embed_dims=[32,32,8,8],heads=[8,4,4,4]):\n        super(Classify, self).__init__()\n        self.embed=embed_dims\n        self.image_size=img_size\n\n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=3, stride=2, in_chans=in_chans,\n                                              embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n\n\n        self.mini_patch_embed1 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[0],\n                                                   embed_dim=embed_dims[1])\n        self.mini_patch_embed2 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2,\n                                                   in_chans=embed_dims[1],\n                                                   embed_dim=embed_dims[2])\n\n        self.self_atten1=SelfAttention(embed_dims[0],heads[0])\n        self.self_atten2=SelfAttention(embed_dims[1],heads[1])\n        self.self_atten3 = SelfAttention(embed_dims[2], heads[2])\n        self.linear_1=nn.Linear(512,100)\n        self.linear_2=nn.Linear(100,3)\n    def forward(self,x):\n        B = x.shape[0]\n        embed_dims = self.embed\n        x1, H1, W1 = self.patch_embed1(x)\n        x2, H2, W2 = self.mini_patch_embed1(x1.permute(0, 2, 1).reshape(B, embed_dims[0], H1, W1))\n\n        x1=self.self_atten1(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten2(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed2(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[1], H1, W1) + x2\n        x2, H2, W2 = self.mini_patch_embed2(x1)\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1=self.self_atten2(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x2 = self.self_atten3(x2)\n        x2 = x2.reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1, H1, W1 = self.patch_embed3(x1)\n        x1 = x1.permute(0, 2, 1).reshape(B, embed_dims[2], H1, W1) + x2\n        x1 = x1.view(x1.shape[0], x1.shape[1], -1).permute(0, 2, 1)\n\n        x1 = self.self_atten3(x1)\n        x1 = x1.reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous()\n\n        x1=x1.flatten(1)\n        tp=x1\n        x1=self.linear_1(x1)\n        x1=self.linear_2(x1)\n        return x1,tp\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.conv1=nn.Conv2d(3,32,kernel_size=3,stride=1,padding=1)\n        self.deconv1 = nn.ConvTranspose2d(32,16, kernel_size=4, stride=2, padding=1)\n\n        self.attn = PatchTransformerEncoder(32,4,512,4)\n\n        self.deconv4 = nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1)\n\n    def forward(self, x):\n\n        x=self.conv1(x)\n\n        x = self.attn(x) + x\n\n        x = F.relu(self.deconv1(x))\n\n        x = torch.sigmoid(self.deconv4(x))\n        return x\n\n\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.classify=Classify()\n\n    def forward(self, x):\n        z = self.encoder(x)\n        x_recon = self.decoder(z)\n        x_classify,embed=self.classify(z)\n        return x_recon,x_classify,embed\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:14:55.826152Z","iopub.execute_input":"2024-06-03T09:14:55.826525Z","iopub.status.idle":"2024-06-03T09:14:55.877665Z","shell.execute_reply.started":"2024-06-03T09:14:55.826496Z","shell.execute_reply":"2024-06-03T09:14:55.876553Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # ***test_model***","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndevice=\"cuda\"\nvae = VAE().to(device)\ninput_tensor = torch.randn(64, 3, 256, 256).to(device)\noutput_tensor,class_tp,embed_tp = vae(input_tensor)\nprint(output_tensor.shape)\nprint(class_tp.shape)\nprint(embed_tp.shape)\nprint(f\"显存使用 (分配): {torch.cuda.memory_allocated(device) / 1024 ** 2:.2f} MB\")\nprint(f\"显存使用 (保留): {torch.cuda.memory_reserved(device) / 1024 ** 2:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:51:47.511089Z","iopub.execute_input":"2024-06-03T03:51:47.511442Z","iopub.status.idle":"2024-06-03T03:51:48.290105Z","shell.execute_reply.started":"2024-06-03T03:51:47.511407Z","shell.execute_reply":"2024-06-03T03:51:48.288892Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 256, 256])\ntorch.Size([64, 3])\ntorch.Size([64, 512])\n显存使用 (分配): 7790.49 MB\n显存使用 (保留): 12574.00 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"del(vae)\ndel(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:52:23.790596Z","iopub.execute_input":"2024-06-03T03:52:23.791400Z","iopub.status.idle":"2024-06-03T03:52:23.795657Z","shell.execute_reply.started":"2024-06-03T03:52:23.791368Z","shell.execute_reply":"2024-06-03T03:52:23.794676Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Train Officially***","metadata":{}},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:15:20.610239Z","iopub.execute_input":"2024-06-03T09:15:20.610950Z","iopub.status.idle":"2024-06-03T09:15:20.614978Z","shell.execute_reply.started":"2024-06-03T09:15:20.610915Z","shell.execute_reply":"2024-06-03T09:15:20.614093Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"epoch=0\nmax_epoch=100\ndataset=train_load(batch_size=32)\nnet=VAE()\ndevice=\"cuda\"\noptimizer=optim.Adam(net.parameters(),lr=0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, min_lr=0.0001)\n\n\nMse_loss=nn.MSELoss()\nGp_loss=nn.CrossEntropyLoss()\nalpha=1\nexp_name=\"vae\"\nos.makedirs(\"./{}\".format(exp_name))\n\nnet.to(device)\nnet.train()\nwhile(epoch<max_epoch):\n    phar=tqdm(total=len(dataset))\n    loss_avg=0\n    z = get_lr(optimizer)\n    for step,(resn,gt,label) in enumerate(dataset):\n        x=resn\n        x=x.to(device)\n        label=label.view(-1).to(device)\n        optimizer.zero_grad()\n        \n        x_recon,pred_label,embed=net(x)\n        loss1=Mse_loss(x,x_recon)\n        loss2=Gp_loss(pred_label,label)\n        loss_avg=(loss_avg*step+loss2)/(step+1)\n        loss=loss2\n        loss.backward()\n        optimizer.step()\n        phar.set_description(\n            \"epoch:{} loss:{} mse_loss:{} class_loss:{} lr:{}\".format(epoch, loss_avg,loss1,loss2,z ))\n        phar.update()\n        \"\"\"\n        try:\n            if(step%51==0):\n                print(pred_label[:10])\n                print(label[:10])\n        except:\n            pass\n        \"\"\"\n    phar.close()\n    epoch+=1\n    state = {\n        \"epoch\": epoch,\n        \"model\": net.state_dict(),\n        \"loss1\": loss1,\n        \"loss2\":loss_avg,\n    }\n    scheduler.step(loss_avg)\n        \n    with open('./{}/latest'.format(exp_name), \"wb\") as f:\n        torch.save(state, f)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:16:14.557678Z","iopub.execute_input":"2024-06-03T09:16:14.558457Z","iopub.status.idle":"2024-06-03T09:37:30.689788Z","shell.execute_reply.started":"2024-06-03T09:16:14.558417Z","shell.execute_reply":"2024-06-03T09:37:30.688241Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb65ed43bed4be5a6cfce1140d0888e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14494e4dd5d413ba83741f0e32605b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b8c16098894e33abc60de5f54fe597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04990838feb54e7eb28964b288bbc537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37a3a38a2f44523b13ffa29c57bf394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74068edd6acf4d4ab44eee381a706162"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m loss_avg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m z \u001b[38;5;241m=\u001b[39m get_lr(optimizer)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step,(resn,gt,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m     23\u001b[0m     x\u001b[38;5;241m=\u001b[39mresn\n\u001b[1;32m     24\u001b[0m     x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mTrainData.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 44\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n","Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mTrainData.get_images\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     37\u001b[0m transform_gt \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size),transforms\u001b[38;5;241m.\u001b[39mToTensor(), transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))])\n\u001b[1;32m     38\u001b[0m input_im \u001b[38;5;241m=\u001b[39m transform_input(input_img)\n\u001b[0;32m---> 39\u001b[0m gt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m z\u001b[38;5;241m=\u001b[39mgt\u001b[38;5;241m-\u001b[39minput_im\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z, input_im, img_id\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2157\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2155\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2159\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for step,(resn,gt,label) in enumerate(dataset):\n    print(label.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # # ***Produse path***","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"./temp\")\nstate=torch.Tensor([1])\nwith open('./{}/latest'.format(\"temp\"), \"wb\") as f:\n        torch.save(state, f)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T07:40:17.026875Z","iopub.execute_input":"2024-06-03T07:40:17.027787Z","iopub.status.idle":"2024-06-03T07:40:17.033202Z","shell.execute_reply.started":"2024-06-03T07:40:17.027753Z","shell.execute_reply":"2024-06-03T07:40:17.032281Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"os.rmdir(\"/kaggle/working/vae\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:16:00.560848Z","iopub.execute_input":"2024-06-03T09:16:00.561228Z","iopub.status.idle":"2024-06-03T09:16:00.568539Z","shell.execute_reply.started":"2024-06-03T09:16:00.561198Z","shell.execute_reply":"2024-06-03T09:16:00.567621Z"},"trusted":true},"execution_count":7,"outputs":[]}]}